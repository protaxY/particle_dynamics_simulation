{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles_number = 120\n",
    "train_valid_ratio = 0.9\n",
    "data_3d = False\n",
    "dim_number = None\n",
    "if data_3d:\n",
    "    dim_number = 3\n",
    "else:\n",
    "    dim_number = 2\n",
    "\n",
    "json_rollouts_directory = Path(\"data/test_data_sample\")\n",
    "h5_rollouts_directory = Path(\"h5_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_particles_number = particles_number\n",
    "dt = 1./30.\n",
    "\n",
    "def combine_stat(stat_0, stat_1):\n",
    "    mean_0, std_0, n_0 = stat_0[:, 0], stat_0[:, 1], stat_0[:, 2]\n",
    "    mean_1, std_1, n_1 = stat_1[:, 0], stat_1[:, 1], stat_1[:, 2]\n",
    "\n",
    "    mean = (mean_0 * n_0 + mean_1 * n_1) / (n_0 + n_1)\n",
    "    std = np.sqrt((std_0**2 * n_0 + std_1**2 * n_1 + \\\n",
    "                   (mean_0 - mean)**2 * n_0 + (mean_1 - mean)**2 * n_1) / (n_0 + n_1))\n",
    "    n = n_0 + n_1\n",
    "    return np.stack([mean, std, n], axis=-1)\n",
    "\n",
    "def filter_json_particles_data(json_data_dict, current_rollout_filename):\n",
    "    good = True\n",
    "    invalid_particle_ids = []\n",
    "    for particle_id, particle in enumerate(json_data_dict[\"particle_positions\"]):\n",
    "        if len(particle) != json_data_dict[\"end_frame\"]+1:\n",
    "            invalid_particle_ids.append(particle_id)\n",
    "    \n",
    "    if len(invalid_particle_ids):\n",
    "        print(current_rollout_filename)\n",
    "        print(invalid_particle_ids)\n",
    "        good = False\n",
    "    \n",
    "    # for invalid_particle_id in invalid_particle_ids:\n",
    "    #     del json_data_dict[\"particle_positions\"][invalid_particle_id]\n",
    "    return good\n",
    "\n",
    "def preprocess_json_data_dict(json_rollouts_directory, particles_number = math.inf):\n",
    "    max_particles_number = particles_number\n",
    "    for current_rollout_filename in os.listdir(json_rollouts_directory):\n",
    "        print(current_rollout_filename)\n",
    "        current_json_rollout_path = json_rollouts_directory / current_rollout_filename\n",
    "        with open(current_json_rollout_path) as f:\n",
    "            json_data_dict = json.load(f) \n",
    "        good = filter_json_particles_data(json_data_dict, current_rollout_filename)\n",
    "        if not good:\n",
    "            os.remove(current_json_rollout_path) \n",
    "            continue\n",
    "        # print(\"particle_positions: \", len(json_data_dict[\"particle_positions\"]))\n",
    "        # max_particles_number = min(len(json_data_dict[\"particle_positions\"]), max_particles_number)\n",
    "        # with open(current_json_rollout_path, \"w\") as f:\n",
    "        #     json.dump(json_data_dict, f)\n",
    "\n",
    "        # print(max_particles_number)\n",
    "\n",
    "    # for current_rollout_filename in os.listdir(json_rollouts_directory):\n",
    "    #     current_json_rollout_path = json_rollouts_directory / current_rollout_filename\n",
    "    #     with open(current_json_rollout_path) as f:\n",
    "    #         json_data_dict = json.load(f) \n",
    "    #     if len(json_data_dict[\"particle_positions\"]) > max_particles_number:\n",
    "    #         number_to_delete = len(json_data_dict[\"particle_positions\"])-max_particles_number\n",
    "    #         del json_data_dict[\"particle_positions\"][-number_to_delete:]\n",
    "    #     with open(current_json_rollout_path, \"w\") as f:\n",
    "    #         json.dump(json_data_dict, f)\n",
    "\n",
    "def make_h5_dataset(h5_rollouts_directory, json_rollouts_directory, train_valid_ratio):\n",
    "    h5_rollouts_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    positions_stat = np.zeros((dim_number, 3))\n",
    "    velocities_stat = np.zeros((dim_number, 3))\n",
    "\n",
    "    objects_number = None\n",
    "    particles_number = None\n",
    "    left_border = None\n",
    "    right_border = None\n",
    "    bottom_border = None\n",
    "    top_border = None\n",
    "\n",
    "    train_rollouts_number = int(len(os.listdir(json_rollouts_directory))*train_valid_ratio)\n",
    "    valid_rollouts_number = len(os.listdir(json_rollouts_directory))-train_rollouts_number\n",
    "\n",
    "    for i, current_rollout_filename in enumerate(os.listdir(json_rollouts_directory)):\n",
    "        current_json_rollout_path = json_rollouts_directory / current_rollout_filename\n",
    "\n",
    "        with open(current_json_rollout_path) as f:\n",
    "            json_data_dict = json.load(f)\n",
    "        \n",
    "        particle_positions = np.array(json_data_dict[\"particle_positions\"])\n",
    "        object_positions = np.array(json_data_dict[\"object_positions\"])\n",
    "        if not (objects_number or \n",
    "                particles_number or \n",
    "                left_border or \n",
    "                right_border or \n",
    "                bottom_border or \n",
    "                top_border):\n",
    "            objects_number = len(object_positions)\n",
    "            particles_number = len(particle_positions)\n",
    "\n",
    "            left_border = json_data_dict[\"left_border\"]\n",
    "            right_border = json_data_dict[\"right_border\"]\n",
    "            bottom_border = json_data_dict[\"bottom_border\"]\n",
    "            top_border = json_data_dict[\"top_border\"]\n",
    "\n",
    "        if dim_number > 2:\n",
    "            particle_positions_new_shape = list(particle_positions.shape)\n",
    "            object_positions_new_shape = list(object_positions.shape)\n",
    "            particle_positions_new_shape[-1] = dim_number\n",
    "            object_positions_new_shape[-1] = dim_number\n",
    "            new_particle_positions = np.zeros(tuple(particle_positions_new_shape))\n",
    "            new_object_positions = np.zeros(tuple(object_positions_new_shape))\n",
    "            new_particle_positions[:, :, :-1] = particle_positions\n",
    "            new_object_positions[:, :, :-1] = object_positions\n",
    "            particle_positions = new_particle_positions\n",
    "            object_positions = new_object_positions\n",
    "\n",
    "        positions = np.vstack((particle_positions, object_positions))\n",
    "        velocities = np.empty(positions.shape)\n",
    "        number = np.empty((dim_number))\n",
    "        number[:] = positions.shape[0]*positions.shape[1]\n",
    "        \n",
    "        current_h5_rollout_directory = None\n",
    "        if i < train_rollouts_number:\n",
    "            (h5_rollouts_directory / \"train\").mkdir(parents=True, exist_ok=True)\n",
    "            current_h5_rollout_directory = h5_rollouts_directory / \"train\" / str(len(os.listdir(h5_rollouts_directory / \"train\")))\n",
    "        else:\n",
    "            (h5_rollouts_directory / \"valid\").mkdir(parents=True, exist_ok=True)\n",
    "            current_h5_rollout_directory = h5_rollouts_directory / \"valid\" / str(len(os.listdir(h5_rollouts_directory / \"valid\")))\n",
    "        current_h5_rollout_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for frame in range(json_data_dict[\"end_frame\"]+1):\n",
    "            current_frame_positions = positions[:, frame]\n",
    "            current_frame_velocities = None\n",
    "            if frame == 0:\n",
    "                current_frame_velocities = np.zeros(current_frame_positions.shape)\n",
    "            else:\n",
    "                previous_frame_positions = positions[:, frame-1]\n",
    "                current_frame_velocities = (current_frame_positions - previous_frame_positions)/dt\n",
    "            velocities[:, frame] = current_frame_velocities\n",
    "\n",
    "            h5_f = h5py.File(current_h5_rollout_directory / (str(frame) + \".h5\"), \"w\")\n",
    "            h5_f.create_dataset(\"positions\", data=current_frame_positions, dtype=\"f4\")\n",
    "            h5_f.create_dataset(\"velocities\", data=current_frame_velocities, dtype=\"f4\")\n",
    "            h5_f.close()\n",
    "\n",
    "        current_rollout_number = int(current_json_rollout_path.stem)\n",
    "\n",
    "        current_positions_stat = np.stack([positions.mean(axis=(0, 1)), positions.std(axis=(0, 1)), number], axis=-1)\n",
    "        current_velocities_stat = np.stack([velocities.mean(axis=(0, 1)), velocities.std(axis=(0, 1)), number], axis=-1)\n",
    "\n",
    "        positions_stat = combine_stat(positions_stat, current_positions_stat)\n",
    "        velocities_stat = combine_stat(velocities_stat, current_velocities_stat)\n",
    "\n",
    "    h5_f = h5py.File(h5_rollouts_directory / \"stat.h5\", \"w\")\n",
    "    h5_f.create_dataset(\"positions\", data=positions_stat, dtype=\"f4\")\n",
    "    h5_f.create_dataset(\"velocities\", data=velocities_stat, dtype=\"f4\")\n",
    "    h5_f.close()\n",
    "\n",
    "    dataset_info = {\"objects_number\": objects_number,\n",
    "                    \"particles_number\": particles_number,\n",
    "                    \"left_border\": left_border,\n",
    "                    \"right_border\": right_border,\n",
    "                    \"bottom_border\": bottom_border,\n",
    "                    \"top_border\": top_border}\n",
    "    file = open(h5_rollouts_directory / \"info.json\", \"w\")\n",
    "    file.write(json.dumps(dataset_info))\n",
    "    file.close()\n",
    "\n",
    "    print(dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483.json\n",
      "250.json\n",
      "1.json\n",
      "540.json\n",
      "374.json\n",
      "395.json\n",
      "31.json\n",
      "501.json\n",
      "152.json\n",
      "108.json\n",
      "30.json\n",
      "533.json\n",
      "467.json\n",
      "211.json\n",
      "168.json\n",
      "219.json\n",
      "40.json\n",
      "138.json\n",
      "42.json\n",
      "238.json\n",
      "14.json\n",
      "528.json\n",
      "93.json\n",
      "8.json\n",
      "196.json\n",
      "13.json\n",
      "318.json\n",
      "420.json\n",
      "61.json\n",
      "337.json\n",
      "433.json\n",
      "328.json\n",
      "158.json\n",
      "566.json\n",
      "418.json\n",
      "291.json\n",
      "129.json\n",
      "102.json\n",
      "597.json\n",
      "130.json\n",
      "210.json\n",
      "80.json\n",
      "49.json\n",
      "112.json\n",
      "185.json\n",
      "441.json\n",
      "6.json\n",
      "363.json\n",
      "277.json\n",
      "95.json\n",
      "47.json\n",
      "392.json\n",
      "396.json\n",
      "167.json\n",
      "287.json\n",
      "472.json\n",
      "235.json\n",
      "275.json\n",
      "249.json\n",
      "52.json\n",
      "294.json\n",
      "194.json\n",
      "227.json\n",
      "557.json\n",
      "492.json\n",
      "445.json\n",
      "51.json\n",
      "460.json\n",
      "160.json\n",
      "111.json\n",
      "146.json\n",
      "148.json\n",
      "572.json\n",
      "443.json\n",
      "187.json\n",
      "15.json\n",
      "110.json\n",
      "24.json\n",
      "513.json\n",
      "55.json\n",
      "164.json\n",
      "233.json\n",
      "524.json\n",
      "265.json\n",
      "183.json\n",
      "41.json\n",
      "54.json\n",
      "213.json\n",
      "105.json\n",
      "121.json\n",
      "244.json\n",
      "193.json\n",
      "375.json\n",
      "90.json\n",
      "338.json\n",
      "307.json\n",
      "126.json\n",
      "439.json\n",
      "220.json\n",
      "204.json\n",
      "366.json\n",
      "144.json\n",
      "593.json\n",
      "583.json\n",
      "38.json\n",
      "226.json\n",
      "493.json\n",
      "505.json\n",
      "21.json\n",
      "39.json\n",
      "465.json\n",
      "115.json\n",
      "48.json\n",
      "246.json\n",
      "71.json\n",
      "485.json\n",
      "477.json\n",
      "345.json\n",
      "43.json\n",
      "4.json\n",
      "188.json\n",
      "330.json\n",
      "181.json\n",
      "549.json\n",
      "425.json\n",
      "3.json\n",
      "409.json\n",
      "276.json\n",
      "552.json\n",
      "526.json\n",
      "403.json\n",
      "66.json\n",
      "484.json\n",
      "0.json\n",
      "177.json\n",
      "68.json\n",
      "432.json\n",
      "411.json\n",
      "316.json\n",
      "170.json\n",
      "452.json\n",
      "299.json\n",
      "510.json\n",
      "109.json\n",
      "302.json\n",
      "136.json\n",
      "317.json\n",
      "379.json\n",
      "147.json\n",
      "476.json\n",
      "230.json\n",
      "96.json\n",
      "399.json\n",
      "114.json\n",
      "498.json\n",
      "228.json\n",
      "229.json\n",
      "344.json\n",
      "245.json\n",
      "247.json\n",
      "521.json\n",
      "560.json\n",
      "142.json\n",
      "122.json\n",
      "321.json\n",
      "290.json\n",
      "234.json\n",
      "156.json\n",
      "380.json\n",
      "331.json\n",
      "434.json\n",
      "545.json\n",
      "446.json\n",
      "240.json\n",
      "332.json\n",
      "298.json\n",
      "259.json\n",
      "447.json\n",
      "447.json\n",
      "[45]\n",
      "490.json\n",
      "253.json\n",
      "5.json\n",
      "70.json\n",
      "9.json\n",
      "515.json\n",
      "373.json\n",
      "508.json\n",
      "203.json\n",
      "580.json\n",
      "184.json\n",
      "585.json\n",
      "397.json\n",
      "561.json\n",
      "415.json\n",
      "270.json\n",
      "601.json\n",
      "385.json\n",
      "215.json\n",
      "600.json\n",
      "567.json\n",
      "118.json\n",
      "266.json\n",
      "381.json\n",
      "356.json\n",
      "53.json\n",
      "322.json\n",
      "497.json\n",
      "212.json\n",
      "346.json\n",
      "73.json\n",
      "468.json\n",
      "285.json\n",
      "94.json\n",
      "313.json\n",
      "35.json\n",
      "348.json\n",
      "588.json\n",
      "312.json\n",
      "312.json\n",
      "[57]\n",
      "389.json\n",
      "117.json\n",
      "75.json\n",
      "225.json\n",
      "225.json\n",
      "[168]\n",
      "273.json\n",
      "581.json\n",
      "295.json\n",
      "596.json\n",
      "444.json\n",
      "377.json\n",
      "383.json\n",
      "28.json\n",
      "166.json\n",
      "45.json\n",
      "456.json\n",
      "387.json\n",
      "149.json\n",
      "495.json\n",
      "12.json\n",
      "370.json\n",
      "511.json\n",
      "33.json\n",
      "199.json\n",
      "471.json\n",
      "286.json\n",
      "286.json\n",
      "[135]\n",
      "50.json\n",
      "367.json\n",
      "556.json\n",
      "333.json\n",
      "141.json\n",
      "217.json\n",
      "217.json\n",
      "[158]\n",
      "343.json\n",
      "527.json\n",
      "589.json\n",
      "77.json\n",
      "429.json\n",
      "208.json\n",
      "516.json\n",
      "76.json\n",
      "352.json\n",
      "531.json\n",
      "546.json\n",
      "349.json\n",
      "504.json\n",
      "359.json\n",
      "271.json\n",
      "140.json\n",
      "254.json\n",
      "82.json\n",
      "448.json\n",
      "422.json\n",
      "354.json\n",
      "288.json\n",
      "320.json\n",
      "320.json\n",
      "[29, 52, 79, 89, 123, 128, 163, 196]\n",
      "128.json\n",
      "463.json\n",
      "462.json\n",
      "92.json\n",
      "159.json\n",
      "107.json\n",
      "449.json\n",
      "507.json\n",
      "450.json\n",
      "539.json\n",
      "191.json\n",
      "178.json\n",
      "172.json\n",
      "174.json\n",
      "457.json\n",
      "255.json\n",
      "473.json\n",
      "308.json\n",
      "570.json\n",
      "570.json\n",
      "[186]\n",
      "451.json\n",
      "201.json\n",
      "371.json\n",
      "406.json\n",
      "237.json\n",
      "327.json\n",
      "488.json\n",
      "577.json\n",
      "274.json\n",
      "296.json\n",
      "551.json\n",
      "364.json\n",
      "519.json\n",
      "252.json\n",
      "586.json\n",
      "32.json\n",
      "280.json\n",
      "573.json\n",
      "384.json\n",
      "536.json\n",
      "262.json\n",
      "192.json\n",
      "496.json\n",
      "382.json\n",
      "46.json\n",
      "125.json\n",
      "469.json\n",
      "351.json\n",
      "260.json\n",
      "400.json\n",
      "466.json\n",
      "104.json\n",
      "131.json\n",
      "161.json\n",
      "480.json\n",
      "198.json\n",
      "263.json\n",
      "17.json\n",
      "223.json\n",
      "89.json\n",
      "564.json\n",
      "221.json\n",
      "101.json\n",
      "305.json\n",
      "106.json\n",
      "503.json\n",
      "417.json\n",
      "2.json\n",
      "576.json\n",
      "509.json\n",
      "278.json\n",
      "360.json\n",
      "113.json\n",
      "326.json\n",
      "335.json\n",
      "116.json\n",
      "523.json\n",
      "27.json\n",
      "7.json\n",
      "525.json\n",
      "202.json\n",
      "478.json\n",
      "216.json\n",
      "426.json\n",
      "272.json\n",
      "79.json\n",
      "405.json\n",
      "72.json\n",
      "599.json\n",
      "25.json\n",
      "133.json\n",
      "165.json\n",
      "339.json\n",
      "200.json\n",
      "437.json\n",
      "532.json\n",
      "124.json\n",
      "135.json\n",
      "334.json\n",
      "413.json\n",
      "598.json\n",
      "431.json\n",
      "568.json\n",
      "103.json\n",
      "455.json\n",
      "195.json\n",
      "241.json\n",
      "123.json\n",
      "180.json\n",
      "143.json\n",
      "590.json\n",
      "182.json\n",
      "575.json\n",
      "548.json\n",
      "84.json\n",
      "435.json\n",
      "231.json\n",
      "267.json\n",
      "100.json\n",
      "365.json\n",
      "10.json\n",
      "454.json\n",
      "512.json\n",
      "324.json\n",
      "547.json\n",
      "502.json\n",
      "355.json\n",
      "11.json\n",
      "243.json\n",
      "410.json\n",
      "319.json\n",
      "419.json\n",
      "145.json\n",
      "404.json\n",
      "26.json\n",
      "424.json\n",
      "293.json\n",
      "268.json\n",
      "162.json\n",
      "261.json\n",
      "479.json\n",
      "347.json\n",
      "594.json\n",
      "83.json\n",
      "574.json\n",
      "514.json\n",
      "398.json\n",
      "461.json\n",
      "487.json\n",
      "500.json\n",
      "127.json\n",
      "81.json\n",
      "190.json\n",
      "78.json\n",
      "301.json\n",
      "269.json\n",
      "151.json\n",
      "132.json\n",
      "323.json\n",
      "242.json\n",
      "358.json\n",
      "18.json\n",
      "464.json\n",
      "388.json\n",
      "390.json\n",
      "222.json\n",
      "236.json\n",
      "137.json\n",
      "98.json\n",
      "336.json\n",
      "58.json\n",
      "538.json\n",
      "206.json\n",
      "57.json\n",
      "602.json\n",
      "550.json\n",
      "257.json\n",
      "506.json\n",
      "311.json\n",
      "544.json\n",
      "361.json\n",
      "62.json\n",
      "284.json\n",
      "258.json\n",
      "22.json\n",
      "175.json\n",
      "353.json\n",
      "169.json\n",
      "486.json\n",
      "218.json\n",
      "368.json\n",
      "578.json\n",
      "209.json\n",
      "63.json\n",
      "155.json\n",
      "562.json\n",
      "173.json\n",
      "20.json\n",
      "36.json\n",
      "304.json\n",
      "224.json\n",
      "150.json\n",
      "87.json\n",
      "438.json\n",
      "205.json\n",
      "251.json\n",
      "518.json\n",
      "163.json\n",
      "541.json\n",
      "256.json\n",
      "423.json\n",
      "157.json\n",
      "372.json\n",
      "309.json\n",
      "587.json\n",
      "16.json\n",
      "153.json\n",
      "34.json\n",
      "474.json\n",
      "37.json\n",
      "19.json\n",
      "436.json\n",
      "394.json\n",
      "553.json\n",
      "119.json\n",
      "393.json\n",
      "584.json\n",
      "442.json\n",
      "408.json\n",
      "569.json\n",
      "592.json\n",
      "386.json\n",
      "139.json\n",
      "314.json\n",
      "378.json\n",
      "23.json\n",
      "453.json\n",
      "283.json\n",
      "99.json\n",
      "310.json\n",
      "69.json\n",
      "207.json\n",
      "134.json\n",
      "67.json\n",
      "86.json\n",
      "91.json\n",
      "579.json\n",
      "554.json\n",
      "97.json\n",
      "369.json\n",
      "369.json\n",
      "[160]\n",
      "197.json\n",
      "186.json\n",
      "470.json\n",
      "44.json\n",
      "407.json\n",
      "189.json\n",
      "391.json\n",
      "481.json\n",
      "494.json\n",
      "315.json\n",
      "558.json\n",
      "535.json\n",
      "530.json\n",
      "563.json\n",
      "214.json\n",
      "459.json\n",
      "289.json\n",
      "64.json\n",
      "176.json\n",
      "350.json\n",
      "88.json\n",
      "421.json\n",
      "430.json\n",
      "232.json\n",
      "458.json\n",
      "522.json\n",
      "571.json\n",
      "325.json\n",
      "555.json\n",
      "376.json\n",
      "239.json\n",
      "591.json\n",
      "491.json\n",
      "340.json\n",
      "440.json\n",
      "534.json\n",
      "59.json\n",
      "279.json\n",
      "306.json\n",
      "329.json\n",
      "65.json\n",
      "120.json\n",
      "300.json\n",
      "542.json\n",
      "499.json\n",
      "416.json\n",
      "342.json\n",
      "582.json\n",
      "362.json\n",
      "179.json\n",
      "565.json\n",
      "292.json\n",
      "401.json\n",
      "428.json\n",
      "282.json\n",
      "248.json\n",
      "171.json\n",
      "543.json\n",
      "281.json\n",
      "297.json\n",
      "60.json\n",
      "154.json\n",
      "517.json\n",
      "482.json\n",
      "595.json\n",
      "357.json\n",
      "341.json\n",
      "303.json\n",
      "29.json\n",
      "475.json\n",
      "489.json\n",
      "412.json\n",
      "56.json\n",
      "414.json\n",
      "427.json\n",
      "520.json\n",
      "85.json\n",
      "537.json\n",
      "559.json\n",
      "264.json\n"
     ]
    }
   ],
   "source": [
    "preprocess_json_data_dict(json_rollouts_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127755\n",
      "[[-7.91367773e-02  2.52118260e+00  1.27755000e+05]\n",
      " [ 1.54715917e-01  2.56570177e+00  1.27755000e+05]]\n",
      "[[ 2.01199627e-02  2.83799128e-01  1.27755000e+05]\n",
      " [-2.32127138e-02  2.50359352e-01  1.27755000e+05]]\n",
      "127755\n",
      "[[-1.65551938e-01  2.57348607e+00  2.55510000e+05]\n",
      " [ 3.77180909e-02  2.60082554e+00  2.55510000e+05]]\n",
      "[[3.15245130e-02 3.36880163e-01 2.55510000e+05]\n",
      " [9.04791320e-04 2.56358944e-01 2.55510000e+05]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m make_h5_dataset(h5_rollouts_directory, json_rollouts_directory, train_valid_ratio)\n",
      "Cell \u001b[0;32mIn[7], line 134\u001b[0m, in \u001b[0;36mmake_h5_dataset\u001b[0;34m(h5_rollouts_directory, json_rollouts_directory, train_valid_ratio)\u001b[0m\n\u001b[1;32m    131\u001b[0m velocities[:, frame] \u001b[39m=\u001b[39m current_frame_velocities\n\u001b[1;32m    133\u001b[0m h5_f \u001b[39m=\u001b[39m h5py\u001b[39m.\u001b[39mFile(current_h5_rollout_directory \u001b[39m/\u001b[39m (\u001b[39mstr\u001b[39m(frame) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.h5\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 134\u001b[0m h5_f\u001b[39m.\u001b[39;49mcreate_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mpositions\u001b[39;49m\u001b[39m\"\u001b[39;49m, data\u001b[39m=\u001b[39;49mcurrent_frame_positions, dtype\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mf4\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    135\u001b[0m h5_f\u001b[39m.\u001b[39mcreate_dataset(\u001b[39m\"\u001b[39m\u001b[39mvelocities\u001b[39m\u001b[39m\"\u001b[39m, data\u001b[39m=\u001b[39mcurrent_frame_velocities, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mf4\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m h5_f\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/h5py/_hl/group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    180\u001b[0m         parent_path, name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39mrsplit(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m         group \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 183\u001b[0m dsid \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmake_new_dset(group, shape, dtype, data, name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    184\u001b[0m dset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mDataset(dsid)\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/h5py/_hl/dataset.py:168\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[1;32m    165\u001b[0m dset_id \u001b[39m=\u001b[39m h5d\u001b[39m.\u001b[39mcreate(parent\u001b[39m.\u001b[39mid, name, tid, sid, dcpl\u001b[39m=\u001b[39mdcpl, dapl\u001b[39m=\u001b[39mdapl)\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m (data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Empty)):\n\u001b[0;32m--> 168\u001b[0m     dset_id\u001b[39m.\u001b[39;49mwrite(h5s\u001b[39m.\u001b[39;49mALL, h5s\u001b[39m.\u001b[39;49mALL, data)\n\u001b[1;32m    170\u001b[0m \u001b[39mreturn\u001b[39;00m dset_id\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "make_h5_dataset(h5_rollouts_directory, json_rollouts_directory, train_valid_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = 'data/test_data_sample/0.json'\n",
    "data_dict = None\n",
    "with open(test_data_path) as f:\n",
    "    data_dict0 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict0[\"particle_positions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict0[\"end_frame\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = h5py.File('../DPI-Net/data/data_DustBox/train/5/237.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(stat[\"velocities\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = h5py.File('../DPI-Net/data/data_DustBox/stat.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"velocities\": shape (2, 3), type \"<f8\">"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats[\"velocities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data([\"positions\", \"velocities\"], '../DPI-Net/data/data_DustBox/train/5/237.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, v = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 2)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_particles 256\n",
      "n_shapes 0\n"
     ]
    }
   ],
   "source": [
    "info_path = 'h5_data_fin/info.json'\n",
    "\n",
    "time_step = 501\n",
    "data_names = ['positions', 'velocities']\n",
    "\n",
    "position_dim = 2\n",
    "\n",
    "phases_dict = {}\n",
    "phases_dict[\"instance_idx\"] = [0, 200, 200+56]\n",
    "phases_dict[\"root_num\"] = [[], []]\n",
    "phases_dict[\"instance\"] = ['dust', 'air_rigid']\n",
    "phases_dict[\"material\"] = ['dust', 'air_rigid']\n",
    "\n",
    "def load_data(data_names, path):\n",
    "    hf = h5py.File(path, 'r')\n",
    "    data = []\n",
    "    for i in range(len(data_names)):\n",
    "        d = np.array(hf.get(data_names[i]))\n",
    "        data.append(d)\n",
    "    hf.close()\n",
    "\n",
    "    return data\n",
    "\n",
    "with open(info_path) as f:\n",
    "    info_dict = json.load(f)\n",
    "\n",
    "tmp_json_data = {'end_frame': time_step-3,\n",
    "                'left_border': info_dict['left_border'],\n",
    "                'right_border': info_dict['right_border'],\n",
    "                'bottom_border': info_dict['bottom_border'],\n",
    "                'top_border': info_dict['top_border'],\n",
    "                'object_positions': [[]for _ in range(phases_dict[\"instance_idx\"][2]-phases_dict[\"instance_idx\"][1])], \n",
    "                'particle_positions': [[]for _ in range(phases_dict[\"instance_idx\"][1]-phases_dict[\"instance_idx\"][0])],\n",
    "                'particle_velocities': [[]for _ in range(phases_dict[\"instance_idx\"][1]-phases_dict[\"instance_idx\"][0])]}\n",
    "\n",
    "for step in range(time_step - 1):\n",
    "    data_path = 'h5_data_fin/valid/0/'+str(step) + '.h5'\n",
    "    data_nxt_path = 'h5_data_fin/valid/0/'+str(step + 1) + '.h5'\n",
    "\n",
    "    data = load_data(data_names, data_path)\n",
    "    data_nxt = load_data(data_names, data_nxt_path)\n",
    "    # velocities_nxt = data_nxt[1]\n",
    "\n",
    "    if step == 0:\n",
    "        positions, velocities = data\n",
    "        n_shapes = 0\n",
    "        scene_params = np.zeros(1)\n",
    "\n",
    "        count_nodes = positions.shape[0]\n",
    "        n_particles = count_nodes - n_shapes\n",
    "        print(\"n_particles\", n_particles)\n",
    "        print(\"n_shapes\", n_shapes)\n",
    "\n",
    "        p_gt = np.zeros((time_step - 1, n_particles + n_shapes, position_dim))\n",
    "        v_nxt_gt = np.zeros((time_step - 1, n_particles + n_shapes, position_dim))\n",
    "\n",
    "        p_pred = np.zeros((time_step - 1, n_particles + n_shapes, position_dim))\n",
    "\n",
    "    # p_gt[step] = data[0][:, -position_dim:]\n",
    "    # v_nxt_gt[step] = data_nxt[1][:, -position_dim:]\n",
    "\n",
    "    p_gt[step] = data[0]\n",
    "    v_nxt_gt[step] = data_nxt[1]\n",
    "\n",
    "    # print(step, np.sum(np.abs(v_nxt_gt[step, :args.n_particles])))\n",
    "\n",
    "    # positions = positions + data[1] * args.dt\n",
    "\n",
    "    # for i, position in enumerate(data[0][:phases_dict[\"instance_idx\"][1]]):\n",
    "    #     tmp_json_data['particle_positions'][i].append(position.tolist())\n",
    "    # for i, position in enumerate(data[0][phases_dict[\"instance_idx\"][1]:phases_dict[\"instance_idx\"][2]]):\n",
    "    #     tmp_json_data['object_positions'][i].append(position.tolist())\n",
    "    # for i, velocity in enumerate(data_nxt[1][:phases_dict[\"instance_idx\"][1]]):\n",
    "    #     tmp_json_data['particle_velocities'][i].append(velocity.tolist())\n",
    "\n",
    "    for i, position in enumerate(p_gt[step, :phases_dict[\"instance_idx\"][1]]):\n",
    "        tmp_json_data['particle_positions'][i].append(position.tolist())\n",
    "    for i, position in enumerate(p_gt[step, phases_dict[\"instance_idx\"][1]:phases_dict[\"instance_idx\"][2]]):\n",
    "        tmp_json_data['object_positions'][i].append(position.tolist())\n",
    "    for i, velocity in enumerate(v_nxt_gt[step, :phases_dict[\"instance_idx\"][1]]):\n",
    "        tmp_json_data['particle_velocities'][i].append(velocity.tolist())\n",
    "    \n",
    "file = open(\"test.json\", \"w\")\n",
    "file.write(json.dumps(tmp_json_data))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
